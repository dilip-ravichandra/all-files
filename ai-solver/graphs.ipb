import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# ==============================
# Simulated Model Results
# ==============================
models = ['Logistic Regression', 'Random Forest', 'XGBoost', 'LSTM']

accuracy = [0.82, 0.89, 0.91, 0.94]
precision = [0.80, 0.88, 0.90, 0.93]
recall = [0.79, 0.87, 0.89, 0.95]
f1 = [0.795, 0.875, 0.895, 0.94]
latency = [15, 20, 25, 40]  # ms (inference time)

# ==============================
# 1. Model Accuracy Comparison
# ==============================
plt.figure(figsize=(8, 5))
plt.bar(models, accuracy, color=['gray', 'green', 'orange', 'blue'])
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0.7, 1.0)
plt.grid(axis='y')
plt.savefig("graphs/accuracy_comparison.png", dpi=300)
plt.show()

# ==============================
# 2. Precision, Recall, F1
# ==============================
x = np.arange(len(models))
width = 0.2

plt.figure(figsize=(8, 5))
plt.bar(x - width, precision, width, label='Precision', color='skyblue')
plt.bar(x, recall, width, label='Recall', color='lightgreen')
plt.bar(x + width, f1, width, label='F1 Score', color='salmon')
plt.xticks(x, models)
plt.title('Precision, Recall, and F1 Score Comparison')
plt.legend()
plt.grid(axis='y')
plt.savefig("graphs/precision_recall_f1.png", dpi=300)
plt.show()

# ==============================
# 3. Latency (Speed)
# ==============================
plt.figure(figsize=(8, 5))
plt.bar(models, latency, color='purple')
plt.title('Model Inference Latency (ms)')
plt.ylabel('Latency (milliseconds)')
plt.grid(axis='y')
plt.savefig("graphs/latency_plot.png", dpi=300)
plt.show()

# ==============================
# 4. Confusion Matrix (Example: LSTM)
# ==============================
y_true = [0, 0, 1, 1, 0, 1, 1, 0, 1, 1]
y_pred = [0, 1, 1, 1, 0, 1, 1, 0, 0, 1]

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - LSTM Model')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig("graphs/confusion_matrix.png", dpi=300)
plt.show()

# ==============================
# 5. Model Training Curve (Example: LSTM)
# ==============================
epochs = range(1, 11)
train_acc = [0.65, 0.72, 0.78, 0.82, 0.86, 0.89, 0.91, 0.93, 0.94, 0.95]
val_acc = [0.63, 0.70, 0.76, 0.81, 0.84, 0.87, 0.90, 0.91, 0.92, 0.94]

plt.figure(figsize=(8, 5))
plt.plot(epochs, train_acc, label='Training Accuracy', marker='o')
plt.plot(epochs, val_acc, label='Validation Accuracy', marker='o')
plt.title('Model Training Curve (LSTM)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.savefig("graphs/model_training_curve.png", dpi=300)
plt.show()

# ==============================
# 6. Feature Importance (Random Forest)
# ==============================
features = ['Automation Level', 'AI Risk Index', 'Skill Match', 'Education Level', 'Experience']
importance = [0.35, 0.25, 0.20, 0.10, 0.10]

plt.figure(figsize=(8, 5))
sns.barplot(x=importance, y=features, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.savefig("graphs/feature_importance.png", dpi=300)
plt.show()
